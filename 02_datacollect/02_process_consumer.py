#!/usr/bin/env python3
"""
FMS Consumer for Real Data Processing
Kafkaë¡œë¶€í„° FMS ì„¼ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì‹ í•˜ê³ , ì´ˆë‹¨ìœ„ë¡œ ë²„í¼ì— ìŒ“ì¸ ë°ì´í„°ë¥¼ HDFSì— ì €ì¥ ë° ì „ì²˜ë¦¬ ì‹¤í–‰
ì „ì²˜ë¦¬ëŠ” ê¸°ì¤€ì¹˜ ë²—ì–´ë‚œ ë°ì´í„°ëŠ” ì—ëŸ¬ ê²½ë¡œì— ì €ì¥
"""

import json
import os
import tempfile
import subprocess
import time
from datetime import datetime
from confluent_kafka import Consumer, KafkaError
import logging
from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Kafka ì„¤ì •
BROKER = "s1:9092,s2:9092,s3:9092"
#TOPIC = "fms-sensor-data"
TOPIC = "topic9"
GROUP_ID = "fms-data-processor"

# HDFS ë„¤ì„ë…¸ë“œ ì£¼ì†Œ
HDFS_NAMENODE = "s1:9000"

# HDFS ê¸°ë³¸ ë””ë ‰í„°ë¦¬
HDFS_RAW_DIR = "/fms/raw-data"
HDFS_ANALYTICS_DIR = "/fms/analytics/data"
HDFS_ERROR_DIR = "/fms/analytics/error"

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FMSDataConsumer:
    def __init__(self):
        self.consumer = Consumer({
            'bootstrap.servers': BROKER,
            'group.id': GROUP_ID,
            'auto.offset.reset': 'earliest'
        })
        self.buffer = []
        self.last_flush_time = time.time()
        self.spark = SparkSession.builder.appName("FMSPreprocessing").getOrCreate()

    def preprocess(self, data_list, y, m, d, hh, mm, ss):
        try:
            df = self.spark.createDataFrame(data_list)

            filtered = df.filter(
                (col("sensor1") >= 0) & (col("sensor1") <= 100) &
                (col("sensor2") >= 0) & (col("sensor2") <= 100) &
                (col("sensor3") >= 0) & (col("sensor3") <= 150) &
                (col("motor1") >= 0) & (col("motor1") <= 2000) &
                (col("motor2") >= 0) & (col("motor2") <= 1500) &
                (col("motor3") >= 0) & (col("motor3") <= 1800) &
                (col("DeviceId") >= 1) & (col("DeviceId") <= 5) &
                (col("isFail").isin(True, False))
            )
            error_df = df.subtract(filtered)

            # ë””ë ‰í„°ë¦¬ ìƒì„±
            analytics_dir = f"{HDFS_ANALYTICS_DIR}/{y}/{m}/{d}/{hh}"
            error_dir = f"{HDFS_ERROR_DIR}/{y}/{m}/{d}/{hh}"
            subprocess.run(["hdfs", "dfs", "-mkdir", "-p", analytics_dir], check=True)
            subprocess.run(["hdfs", "dfs", "-mkdir", "-p", error_dir], check=True)

            processed_path = f"{analytics_dir}/processeddata_{hh}{mm}{ss}.json"
            error_path = f"{error_dir}/processeddata_{hh}{mm}{ss}.json"

            filtered.coalesce(1).write.mode("overwrite").json(f"hdfs://{HDFS_NAMENODE}{processed_path}")
            if error_df.count() > 0:
                error_df.coalesce(1).write.mode("overwrite").json(f"hdfs://{HDFS_NAMENODE}{error_path}")
                logger.info(f"âš  ì—ëŸ¬ ë°ì´í„° ì €ì¥: {error_path}")

            logger.info(f"âœ” ì „ì²˜ë¦¬ ì •ìƒ ë°ì´í„° ì €ì¥: {processed_path}")
        except Exception as e:
            logger.error(f"ì „ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    def write_buffer_to_hdfs(self):
        if not self.buffer:
            logger.info("ë²„í¼ì— ë°ì´í„° ì—†ì–´ ì €ì¥ ì•ˆí•¨")
            return

        now = datetime.now()
        y, m, d, hh, mm, ss = now.strftime("%Y %m %d %H %M %S").split()

        raw_dir = f"{HDFS_RAW_DIR}/{y}/{m}/{d}/{hh}"
        filename = f"sensor-data_{now.strftime('%Y%m%d_%H%M%S')}.json"
        hdfs_path = f"{raw_dir}/{filename}"

        # ë””ë ‰í„°ë¦¬ ìƒì„±
        try:
            subprocess.run(["hdfs", "dfs", "-mkdir", "-p", raw_dir], check=True)
        except subprocess.CalledProcessError as e:
            logger.error(f"HDFS ë””ë ‰í„°ë¦¬ ìƒì„± ì‹¤íŒ¨: {e.stderr}")

        # ì„ì‹œíŒŒì¼ì— ë²„í¼ ì €ì¥ í›„ HDFS ì—…ë¡œë“œ
        with tempfile.NamedTemporaryFile("w", delete=False) as tmp:
            for rec in self.buffer:
                tmp.write(json.dumps(rec) + "\n")
            tmp_path = tmp.name

        try:
            subprocess.run(["hdfs", "dfs", "-put", "-f", tmp_path, hdfs_path], check=True)
            logger.info(f"ğŸ“ ì›ë³¸ ì €ì¥ ì™„ë£Œ: {hdfs_path}")
        except subprocess.CalledProcessError as e:
            logger.error(f"HDFS ì €ì¥ ì‹¤íŒ¨: {e.stderr}")
        finally:
            os.remove(tmp_path)

        # ë²„í¼ ë°ì´í„°ë¡œ ì „ì²˜ë¦¬ ì‹¤í–‰
        self.preprocess(self.buffer, y, m, d, hh, mm, ss)

        self.buffer.clear()
        self.last_flush_time = time.time()

    def process_message(self, msg):
        try:
            raw_value = msg.value().decode('utf-8')
            logger.info(f"[RAW INPUT] {raw_value}")
            data = json.loads(raw_value)

            # ìˆ«ì í•„ë“œ ëª¨ë‘ floatë¡œ íƒ€ì… í†µì¼
            for key in ['sensor1', 'sensor2', 'sensor3', 'motor1', 'motor2', 'motor3']:
                if key in data:
                    data[key] = float(data[key])

            # DeviceIdëŠ” intë¡œ í†µì¼
            if 'DeviceId' in data:
                data['DeviceId'] = int(data['DeviceId'])

            # isFailì€ bool ìœ ì§€ (json.loadsì—ì„œ ìë™ ì²˜ë¦¬ë¨)

            self.buffer.append(data)
        except json.JSONDecodeError as e:
            logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        except Exception as e:
            logger.error(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    def run(self):
        logger.info("FMS Consumer ì‹œì‘...")
        logger.info(f"êµ¬ë… í† í”½: {TOPIC}")

        self.consumer.subscribe([TOPIC])

        try:
            while True:
                start_time = time.time()
                # 1ì´ˆ ë™ì•ˆ ë©”ì‹œì§€ ì­‰ ë°›ê¸°
                while time.time() - start_time < 1:
                    msg = self.consumer.poll(0.1)
                    if msg is None:
                        continue
                    elif msg.error():
                        if msg.error().code() == KafkaError._PARTITION_EOF:
                            continue
                        else:
                            logger.error(f"Consumer error: {msg.error()}")
                            continue
                    else:
                        self.process_message(msg)

                # 1ì´ˆê°„ ë°›ì€ ë©”ì‹œì§€ ëª¨ë‘ ì²˜ë¦¬(ì €ì¥+ì „ì²˜ë¦¬)
                self.write_buffer_to_hdfs()

        except KeyboardInterrupt:
            logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        finally:
            self.write_buffer_to_hdfs()
            self.consumer.close()
            self.spark.stop()
            logger.info("Consumer ì¢…ë£Œ")

if __name__ == "__main__":
    consumer = FMSDataConsumer()
    consumer.run()

