#!/usr/bin/env python3
"""
FMS Consumer for Real Data Processing
Kafkaë¡œë¶€í„° FMS ì„¼ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì‹ í•˜ê³ , ìˆ˜ì‹  ì¦‰ì‹œ HDFSì— ì €ì¥ (íŒŒì¼ëª…ì€ íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜)
"""
import json
import os
import tempfile
import subprocess
from datetime import datetime
from confluent_kafka import Consumer, KafkaError
import logging

# Kafka ì„¤ì •
BROKER = "s1:9092,s2:9092,s3:9092"
TOPIC = "topic9"
GROUP_ID = "fms-data-processor"

# HDFS ë””ë ‰í„°ë¦¬
HDFS_DIR = "/fms/raw-data/"

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FMSDataConsumer:
    def __init__(self):
        self.consumer = Consumer({
            'bootstrap.servers': BROKER,
            'group.id': GROUP_ID,
            'auto.offset.reset': 'earliest'
        })
        self.buffer = []

    def write_buffer_to_hdfs(self):
        """ë²„í¼ ë‚´ìš©ì„ íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì¼ë¡œ HDFSì— ì €ì¥"""
        if not self.buffer:
            logger.info("ë²„í¼ ë¹„ì–´ìˆìŒ, ì €ì¥ ê±´ë„ˆëœ€")
            return

        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"sensor-data-{timestamp_str}.json"
        hdfs_path = os.path.join(HDFS_DIR, filename)

        try:
            with tempfile.NamedTemporaryFile('w', delete=False) as tmp:
                for record in self.buffer:
                    tmp.write(json.dumps(record) + '\n')
                tmp_path = tmp.name

            subprocess.run(
                ["hdfs", "dfs", "-put", tmp_path, hdfs_path],
                check=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            logger.info(f"ğŸ“ HDFS ì €ì¥ ì™„ë£Œ: {hdfs_path}")
        except subprocess.CalledProcessError as e:
            logger.error(f"HDFS ì—…ë¡œë“œ ì‹¤íŒ¨: {e.stderr.strip()}")
        finally:
            try:
                os.remove(tmp_path)
            except Exception:
                pass
            self.buffer.clear()

    def process_message(self, msg):
        """ë©”ì‹œì§€ë¥¼ íŒŒì‹±í•˜ì—¬ ë²„í¼ì— ì €ì¥"""
        try:
            raw_value = msg.value().decode('utf-8')
            logger.info(f"[RAW INPUT] {raw_value}")

            data = json.loads(raw_value)
            self.buffer.append(data)
        except json.JSONDecodeError as e:
            logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        except Exception as e:
            logger.error(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

    def run(self):
        """Consumer ì‹¤í–‰"""
        logger.info("FMS Data Consumer ì‹œì‘...")
        logger.info(f"êµ¬ë… í† í”½: {TOPIC}")

        self.consumer.subscribe([TOPIC])

        try:
            while True:
                msg = self.consumer.poll(1.0)

                if msg is None:
                    continue
                elif msg.error():
                    if msg.error().code() == KafkaError._PARTITION_EOF:
                        continue
                    else:
                        logger.error(f"Consumer error: {msg.error()}")
                        continue
                else:
                    self.process_message(msg)
                    self.write_buffer_to_hdfs()  # ìˆ˜ì‹  ì¦‰ì‹œ ì €ì¥

        except KeyboardInterrupt:
            logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        finally:
            self.write_buffer_to_hdfs()
            self.consumer.close()
            logger.info("Consumer ì¢…ë£Œ")

if __name__ == "__main__":
    consumer = FMSDataConsumer()
    consumer.run()

